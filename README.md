# ğŸ§ Speaker Identification & Semantic Speech Analysis

> An end-to-end Deep Learning + NLP web application that combines **Speaker Recognition** and **Semantic Speech Understanding** in a production-ready Flask architecture.

---

## Overview

This project integrates **Audio Deep Learning** and **Natural Language Processing** into a unified intelligent pipeline.

From a single audio recording, the system can:

| Capability | Technology |
|---|---|
| ğŸ™ï¸ Identify the Speaker | CNN-based model |
| ğŸ“ Transcribe Speech to Text | OpenAI Whisper |
| ğŸ§  Extract Keywords | KeyBERT |
| ğŸ“„ Generate an Automatic Summary | LSA â€“ Sumy |

> It demonstrates how **Deep Learning models and NLP pipelines can be orchestrated together in a clean, modular web application.**

---

## Key Features

- End-to-end audio-to-semantic analysis pipeline
- CNN-based speaker classification using MFCC features
- State-of-the-art Whisper transcription
- Semantic keyword extraction
- Extractive summarization
- Modular Flask architecture
- Production-ready structure

---

## System Architecture

```
Audio Input
    â†“
Audio Preprocessing  Â·  Librosa + MFCC
    â†“
CNN Speaker Classification  Â·  TensorFlow / Keras
    â†“
Whisper Transcription  Â·  OpenAI Whisper
    â†“
Keyword Extraction  Â·  KeyBERT
    â†“
Summarization  Â·  LSA (Sumy)
    â†“
Flask Web Interface
```

---

## Technologies Used

### ğŸ§ Audio & Deep Learning

| Library | Role |
|---|---|
| **TensorFlow / Keras** | CNN speaker classification model |
| **Librosa** | MFCC feature extraction |
| **NumPy** | Feature normalization |

### ğŸ§  NLP & Speech

| Library | Role |
|---|---|
| **OpenAI Whisper** | Speech-to-text transcription |
| **KeyBERT** | Keyword extraction |
| **Sumy (LSA)** | Extractive summarization |
| **NLTK** | Sentence tokenization |

### ğŸŒ Web Application

| Tool | Role |
|---|---|
| **Flask** | Backend framework |
| **HTML / CSS** | Frontend interface |

---

## Project Structure

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py             # Flask app factory & model loading
â”‚   â”œâ”€â”€ routes.py               # HTTP routes
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ speaker_service.py
â”‚   â”‚   â”œâ”€â”€ transcription_service.py
â”‚   â”‚   â”œâ”€â”€ keyword_service.py
â”‚   â”‚   â””â”€â”€ summarization_service.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ audio_processing.py
â”‚   â””â”€â”€ templates/
â”‚       â”œâ”€â”€ home.html
â”‚       â””â”€â”€ index.html
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ final_model.h5
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ wav_transform.py
â”‚
â”œâ”€â”€ predict.py
â”œâ”€â”€ config.py
â”œâ”€â”€ run.py
â””â”€â”€ requirements.txt
```

---

## How It Works

### 1 â€” Speaker Identification

Audio is converted to WAV format if required, then MFCC features are extracted using Librosa. Features are normalized using pre-saved mean & standard deviation values, and the CNN model predicts the speaker class.

### 2 â€” Speech Transcription

Audio is passed directly to Whisper, which performs automatic language detection and outputs clean text transcription.

### 3 â€” Keyword Extraction

KeyBERT identifies semantically meaningful keywords and returns the top N most relevant words and phrases from the transcription.

### 4 â€” Automatic Summarization

LSA (Latent Semantic Analysis) selects the most informative sentences from the transcription to produce a concise extractive summary.

---

## Installation & Usage

### 1 â€” Clone the Repository

```bash
git clone https://github.com/your-username/your-repository.git
cd your-repository
```

### 2 â€” Create a Virtual Environment

```bash
python -m venv venv
venv\Scripts\activate   # Windows
```

### 3 â€” Install Dependencies

```bash
pip install -r requirements.txt
```

### 4 â€” Add Required Model Files

Ensure the following files exist before running:

```
models/final_model.h5
label_mapping.npy
mean.npy
std.npy
```

### 5 â€” Run the Application

```bash
python run.py
```

### 6 â€” Open in Your Browser

```
http://localhost:5000/
```

---

## Design Decisions

**Heavy models loaded once at startup** â€” Whisper, KeyBERT, and the CNN model are initialized once when the app boots, avoiding per-request overhead.

**Clean modular architecture** â€” The codebase separates concerns clearly:
- `services/` â†’ Business logic
- `utils/` â†’ Reusable helper functions
- `scripts/` â†’ Offline training tools

**Centralized configuration** â€” All settings live in `config.py` for easy environment management.

**Clear backend/frontend separation** â€” Flask serves as a pure API layer; HTML/CSS handles presentation independently.

---

## Learning Outcomes

This project demonstrates practical experience across:

- End-to-end ML system integration
- Audio feature engineering (MFCC)
- CNN-based speaker classification
- Modern speech models (Whisper)
- NLP semantic processing
- Production-ready Flask architecture

---

## Future Improvements

- [ ] Add speaker verification (1:1 matching)
- [ ] Real-time streaming transcription
- [ ] Improve transcription formatting
- [ ] Dockerize the application
- [ ] Deploy to AWS / GCP / Azure
- [ ] Replace LSA with Transformer-based summarization
- [ ] Add confidence scores & analytics dashboard

---

## Author

**Achraf Moualem**  
AI & Data Science Student  
*Interested in AI Engineering, Speech Processing & Generative AI*
